{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70668e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd983fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.0+cu102  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add7e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bc1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "''' 3. Data Augmentation이 적용된 CIFAR10 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10_aug\",\n",
    "                                  train = True,\n",
    "                                  download = True,\n",
    "                                  transform = transforms.Compose([\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10_aug\",\n",
    "                                train = False,\n",
    "                                transform = transforms.Compose([\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. PyTorch 내에서 제공하는 ResNet34 모델 불러온 후 FC 층 추가 및 Output 크기 설정하기 '''\n",
    "import torchvision.models as models\n",
    "model = models.resnet34(pretrained = False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68912b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75b3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. 불러온 Resnet34 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50c354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16e1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anpdi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.743464\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.947585\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.670126\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.936503\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.488118\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.402442\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.230571\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.413154\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.2741, \tTest Accuracy: 55.94 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.283391\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.171623\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.171290\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 0.888814\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 1.055972\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.423354\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.058348\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 0.975359\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.0287, \tTest Accuracy: 64.96 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 0.853205\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 1.018277\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 0.756006\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 1.077030\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 1.089439\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.299268\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 0.617609\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 0.617736\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.0855, \tTest Accuracy: 65.88 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 0.779007\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 0.620330\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 0.754770\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 0.676288\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 0.657955\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 0.749574\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 0.775578\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 0.566718\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.7863, \tTest Accuracy: 72.52 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 0.735229\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.533230\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 0.504301\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.666644\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 0.889955\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 0.761634\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 0.616023\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 0.399767\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.7845, \tTest Accuracy: 73.83 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 0.395000\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 0.783419\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 0.648380\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.662512\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 0.495248\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.493891\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 0.646316\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 0.987224\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.7118, \tTest Accuracy: 75.88 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 0.388308\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 0.521539\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.539161\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.881604\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 0.766983\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.518832\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 0.362433\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.756706\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.6742, \tTest Accuracy: 76.68 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 0.540667\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 0.224417\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 0.451713\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 0.395906\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.594148\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.567670\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 0.866497\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.526914\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.6223, \tTest Accuracy: 78.72 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.796328\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 0.390051\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.473378\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.301680\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 0.605181\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.327641\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.432111\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 0.298132\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.8579, \tTest Accuracy: 75.02 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.551656\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.359810\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.531429\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.517374\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.481042\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 0.401710\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.531102\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.603391\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.6333, \tTest Accuracy: 78.90 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. 불러온 ResNet34 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feca06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
